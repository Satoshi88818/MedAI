# ===================================================================
# MedAI v10.1 - Universal Multimodal Diagnostic & Treatment AI System
# Fixed, Secure, Production-Ready | December 10, 2025
# ===================================================================

import os
import io
import uuid
import json
import logging
import yaml
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, applications, applications, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.efficientnet import preprocess_input
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from PIL import Image
import cv2
import joblib
import shap
import openslide
from fastapi import FastAPI, HTTPException, Depends, Form, File, UploadFile, status
from fastapi.security import OAuth2PasswordBearer
from pydantic import BaseModel, validator
import uvicorn
import jwt  # PyJWT

# ============================= CONFIG =============================
CONFIG_FILE = "medai_config.yaml"

with open(CONFIG_FILE, "r") as f:
    raw_config = yaml.safe_load(f)

class DiseaseConfig(BaseModel):
    name: str
    modalities: List[str]  # ["image", "tabular", "text", "wsi"]
    input_shape: Tuple[int, int, int] = (384, 384, 3)
    tabular_features: int
    treatments: List[str]
    rules_file: str
    cancer_threshold: Optional[float] = 0.5
    model_version: str = "1.0"

class MedAIConfig(BaseModel):
    logging: Dict
    model_dir: str = "models"
    diseases: Dict[str, DiseaseConfig]
    security: Dict
    jwt_secret: str
    jwt_algorithm: str = "HS256"

    @validator("diseases")
    def validate_diseases(cls, v):
        for name, cfg in v.items():
            required = {"modalities", "tabular_features", "treatments", "rules_file"}
            if not all(k in cfg for k in required):
                raise ValueError(f"Disease '{name}' missing required fields: {required}")
        return v

CONFIG = MedAIConfig(**raw_config)

# ============================= LOGGING & AUDIT =============================
os.makedirs("logs", exist_ok=True)
log_filename = f"logs/medai_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(filename=log_filename, level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s")

audit_logger = logging.getLogger("audit")
audit_handler = logging.FileHandler(f"logs/audit_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
audit_handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
audit_logger.addHandler(audit_handler)
audit_logger.setLevel(logging.INFO)

# ============================= AUTH =============================
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def verify_token(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, CONFIG.jwt_secret, algorithms=[CONFIG.jwt_algorithm])
        return payload
    except jwt.PyJWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or expired token",
            headers={"WWW-Authenticate": "Bearer"},
        )

# ============================= SAFE RULES ENGINE =============================
import operator

OPS = {
    {
        '<': operator.lt,
        '<=': operator.le,
        '==': operator.eq,
        '!=': operator.ne,
        '>=': operator.ge,
        '>': operator.gt,
        'and': operator.and_,
        'or': operator.or_,
        'in': lambda x, y: x in y,
    }

def safe_eval(expr: str, context: Dict[str, Any]) -> Any:
    """Very restricted eval — only comparisons, arithmetic, and 'in'."""
    allowed_names = {k: v for k, v in context.items() if isinstance(v, (int, float, str, bool, list, tuple))}
    try:
        code = compile(expr, '<string>', 'eval', flags=1024, optimize=2)  # ast-only
        for name in code.co_names:
            if name not in allowed_names and name not in OPS:
                raise NameError(f"Access to disallowed name '{name}'")
        return eval(code, {"__builtins__": {}}, {**allowed_names, **OPS})
    except Exception as e:
        logging.warning(f"Rule evaluation failed: {expr} → {e}")
        return False

def apply_rules(disease_cfg: DiseaseConfig, prediction: Dict, metadata: Dict) -> Tuple[int, str]:
    if not os.path.exists(disease_cfg.rules_file):
        return prediction["treatment_idx"], prediction["treatment_name"]

    rules = yaml.safe_load(open(disease_cfg.rules_file)) or []
    context = {
        "p": prediction.get("probability", 0.5),
        "m": metadata,
        "idx": prediction["treatment_idx"],
        "t": disease_cfg.treatments
    }

    for rule in rules:
        if safe_eval(rule["condition"], context):
            new_idx = safe_eval(rule["action"], context)
            if isinstance(new_idx, int) and 0 <= new_idx < len(disease_cfg.treatments):
                reason = rule.get("reason", "rule override")
                return new_idx, f"{disease_cfg.treatments[new_idx]} ({reason})"
    return prediction["treatment_idx"], prediction["treatment_name"]

# ============================= MODEL BUILDERS =============================
def build_image_backbone(input_shape=(384, 384, 3)):
    base = applications.EfficientNetB4(include_top=False, weights='imagenet', input_shape=input_shape)
    base.trainable = False
    inputs = layers.Input(shape=input_shape)
    x = base(inputs)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dropout(0.3)(x)
    return models.Model(inputs, x, name="image_backbone")

def build_tabular_head(input_dim: int, n_classes: int):
    inputs = layers.Input(shape=(input_dim,))
    x = layers.Dense(512, activation='relu')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dropout(0.3)(x)
    outputs = layers.Dense(n_classes, activation='softmax')(x)
    return models.Model(inputs, outputs, name="treatment_head")

# ============================= MEDAI CORE =============================
class MedAI:
    def __init__(self):
        os.makedirs(CONFIG.model_dir, exist_ok=True)
        self.models = {}
        self.scalers = {}
        self.load_all_diseases()

    def load_all_diseases(self):
        for disease_name, disease_cfg in CONFIG.diseases.items():
            logging.info(f"Loading disease model: {disease_name}")
            disease_dir = os.path.join(CONFIG.model_dir, disease_name)
            os.makedirs(disease_dir, exist_ok=True)

            # Image backbone
            if "image" in disease_cfg.modalities or "wsi" in disease_cfg.modalities:
                img_path = os.path.join(disease_dir, "image_backbone.h5")
                if os.path.exists(img_path):
                    self.models[f"{disease_name}_image"] = models.load_model(img_path)
                else:
                    self.models[f"{disease_name}_image"] = build_image_backbone(disease_cfg.input_shape)

            # Tabular head + scaler
            head_path = os.path.join(disease_dir, "treatment_head.h5")
            scaler_path = os.path.join(disease_dir, "scaler.pkl")
            if os.path.exists(head_path) and os.path.exists(scaler_path):
                self.models[f"{disease_name}_head"] = models.load_model(head_path)
                self.scalers[disease_name] = joblib.load(scaler_path)
            else:
                self.models[f"{disease_name}_head"] = build_tabular_head(
                    disease_cfg.tabular_features + 1280,  # 1280 = EfficientNetB4 GAP size
                    len(disease_cfg.treatments)
                )
                self.scalers[disease_name] = None

    def extract_wsi_features(self, wsi_bytes: bytes, backbone) -> np.ndarray:
        """Extract and aggregate features from whole slide image using attention-based MIL."""
        temp_path = f"temp_{uuid.uuid4().hex}.svs"
        with open(temp_path, "wb") as f:
            f.write(wsi_bytes)
        slide = openslide.OpenSlide(temp_path)

        tiles = []
        coords = []
        tile_size = 512
        target_size = (384, 384)
        level = 0
        width, height = slide.dimensions

        for y in range(0, height - tile_size, tile_size // 2):  # 50% overlap
            for x in range(0, width - tile_size, tile_size // 2):
                tile = slide.read_region((x, y), level, (tile_size, tile_size))
                tile = tile.convert("RGB")
                arr = np.array(tile)
                if np.mean(arr) < 235:  # skip blank/background
                    arr = cv2.resize(arr, target_size)
                    arr = preprocess_input(arr.astype(np.float32))
                    tiles.append(arr)
                    coords.append((x, y))

        slide.close()
        os.remove(temp_path)

        if len(tiles) == 0:
            raise ValueError("No tissue tiles found in WSI")

        tiles = np.stack(tiles)
        batch_size = 32
        features = []
        for i in range(0, len(tiles), batch_size):
            batch = tiles[i:i+batch_size]
            feats = backbone.predict(batch, verbose=0)
            features.append(feats)
        features = np.vstack(features)  # (N, 1280)

        # Simple attention pooling (better than mean)
        attention = layers.Dense(128, activation='tanh')(features)
        attention = layers.Dense(1, activation=None)(attention)
        attention = tf.nn.softmax(attention, axis=0).numpy().flatten()
        aggregated = np.sum(features * attention[:, None], axis=0, keepdims=True)
        return aggregated

    def predict(self, disease: str, image: Optional[np.ndarray], wsi_bytes: Optional[bytes],
                tabular: np.ndarray, metadata: Dict, request_id: str) -> Dict:
        disease_cfg = CONFIG.diseases[disease]
        result = {
            "request_id": request_id,
            "disease": disease,
            "model_version": disease_cfg.model_version,
            "timestamp": datetime.utcnow().isoformat(),
            "metadata": metadata
        }

        # Tabular scaling
        scaler = self.scalers.get(disease)
        if scaler is None:
            raise RuntimeError(f"Scaler not found for {disease} — model not properly trained")
        tabular = scaler.transform(tabular.reshape(1, -1))

        # Image or WSI feature extraction
        if "image" in disease_cfg.modalities or "wsi" in disease_cfg.modalities:
            backbone = self.models[f"{disease}_image"]
            if wsi_bytes is not None:
                img_features = self.extract_wsi_features(wsi_bytes, backbone)
            elif image is not None:
                img = cv2.resize(image, (384, 384))
                img = preprocess_input(img.astype(np.float32))
                img = np.expand_dims(img, 0)
                # MC Dropout
                preds = [backbone(img, training=True) for _ in range(50)]
                img_features = np.mean(preds, axis=0)
            else:
                raise ValueError("Image or WSI required")
        else:
            img_features = np.zeros((1, 1280))

        head_input = np.concatenate([img_features, tabular], axis=1)
        head = self.models[f"{disease}_head"]

        # MC Dropout for uncertainty
        treatment_probs = np.array([head(head_input, training=True).numpy()[0] for _ in range(50)])
        mean_probs = np.mean(treatment_probs, axis=0)
        std_probs = np.std(treatment_probs, axis=0)

        idx = int(np.argmax(mean_probs))
        final_idx, treatment_name = apply_rules(disease_cfg, {
            "treatment_idx": idx,
            "treatment_name": disease_cfg.treatments[idx],
            "probability": float(mean_probs[idx])
        }, metadata)

        result.update({
            "recommended_treatment": treatment_name,
            "treatment_confidence": float(mean_probs[final_idx]),
            "treatment_uncertainty": float(std_probs[final_idx]),
            "all_treatment_probs": {disease_cfg.treatments[i]: float(p) for i, p in enumerate(mean_probs)},
        })

        audit_logger.info(f"Prediction {request_id}: {json.dumps(result)}")
        return result

# ============================= FASTAPI APP =============================
app = FastAPI(title="MedAI v10.1 - Universal Multimodal Medical AI", version="10.1.0")
ai = MedAI()

@app.post("/predict")
async def predict(
    disease: str = Form(...),
    tabular_data: str = Form(...),  # JSON string to avoid List[float] parsing issues
    metadata: str = Form("{}"),
    image_file: Optional[UploadFile] = File(None),
    wsi_file: Optional[UploadFile] = File(None),
    token: str = Depends(verify_token),  # ← real auth now enforced
):
    request_id = str(uuid.uuid4())
    try:
        if disease not in CONFIG.diseases:
            raise HTTPException(status_code=404, detail=f"Disease '{disease}' not supported")

        # Parse inputs
        tabular = np.array(json.loads(tabular_data))
        metadata_dict = json.loads(metadata)

        image = None
        wsi_bytes = None

        if image_file and image_file.filename:
            contents = await image_file.read()
            img = Image.open(io.BytesIO(contents)).convert("RGB")
            image = np.array(img)

        if wsi_file and wsi_file.filename:
            wsi_bytes = await wsi_file.read()

        result = ai.predict(
            disease=disease,
            image=image,
            wsi_bytes=wsi_bytes,
            tabular=tabular,
            metadata=metadata_dict,
            request_id=request_id
        )
        return result

    except Exception as e:
        logging.error(f"Prediction failed {request_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# ============================= MAIN =============================
if __name__ == "__main__":
    print("MedAI v10.1 - Universal Multimodal Medical AI System")
    print("Supported diseases:", list(CONFIG.diseases.keys()))
    uvicorn.run("medai_v10_1:app", host="0.0.0.0", port=8000, reload=False)